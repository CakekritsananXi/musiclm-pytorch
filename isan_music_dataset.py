"""
Isan Music Dataset and Preprocessing Module
‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°
‡∏û‡∏¥‡∏ì ‡πÅ‡∏Ñ‡∏ô ‡πÇ‡∏´‡∏ß‡∏î ‡πÇ‡∏õ‡∏á‡∏•‡∏≤‡∏á
"""

import torch
import torchaudio
import numpy as np
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import librosa
from dataclasses import dataclass

@dataclass
class IsanInstrumentConfig:
    """Configuration for traditional Isan instruments"""
    name: str
    frequency_range: Tuple[float, float]  # min, max frequency in Hz
    sample_rate: int = 16000
    n_fft: int = 2048
    hop_length: int = 512
    n_mels: int = 128
    
# Traditional Isan instruments configuration
ISAN_INSTRUMENTS = {
    '‡∏û‡∏¥‡∏ì': IsanInstrumentConfig(
        name='‡∏û‡∏¥‡∏ì',
        frequency_range=(200, 2000),  # String instrument with mid-range frequencies
        sample_rate=16000
    ),
    '‡πÅ‡∏Ñ‡∏ô': IsanInstrumentConfig(
        name='‡πÅ‡∏Ñ‡∏ô',
        frequency_range=(80, 800),    # Mouth organ with low to mid frequencies
        sample_rate=16000
    ),
    '‡πÇ‡∏´‡∏ß‡∏î': IsanInstrumentConfig(
        name='‡πÇ‡∏´‡∏ß‡∏î',
        frequency_range=(100, 1200),  # Drum-like percussion
        sample_rate=16000
    ),
    '‡πÇ‡∏õ‡∏á‡∏•‡∏≤‡∏á': IsanInstrumentConfig(
        name='‡πÇ‡∏õ‡∏á‡∏•‡∏≤‡∏á',
        frequency_range=(150, 1500),  # Bamboo percussion
        sample_rate=16000
    )
}

class IsanMusicDataset(torch.utils.data.Dataset):
    """
    Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô
    Dataset for Isan traditional music generation
    """
    
    def __init__(
        self,
        data_path: str,
        instrument: str = '‡∏û‡∏¥‡∏ì',
        sample_rate: int = 16000,
        duration: float = 10.0,  # seconds
        transform=None
    ):
        self.data_path = Path(data_path)
        self.instrument = instrument
        self.sample_rate = sample_rate
        self.duration = duration
        self.samples_per_track = int(sample_rate * duration)
        self.transform = transform
        
        if instrument not in ISAN_INSTRUMENTS:
            raise ValueError(f"Instrument {instrument} not supported. Available: {list(ISAN_INSTRUMENTS.keys())}")
        
        self.instrument_config = ISAN_INSTRUMENTS[instrument]
        self.audio_files = self._load_audio_files()
        
    def _load_audio_files(self) -> List[Path]:
        """‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô"""
        audio_extensions = ['.wav', '.mp3', '.flac', '.m4a']
        audio_files = []
        
        for ext in audio_extensions:
            audio_files.extend(self.data_path.rglob(f'*{ext}'))
        
        if not audio_files:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {self.instrument} ‡πÉ‡∏ô {self.data_path}")
            print(f"‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤...")
            self._create_sample_audio()
            return self._load_audio_files()
            
        return audio_files
    
    def _create_sample_audio(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤"""
        sample_dir = self.data_path / self.instrument
        sample_dir.mkdir(parents=True, exist_ok=True)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏î‡∏ô‡∏ï‡∏£‡∏µ
        duration_samples = int(self.sample_rate * 2)  # 2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
        t = np.linspace(0, 2, duration_samples)
        
        if self.instrument == '‡∏û‡∏¥‡∏ì':
            # ‡∏û‡∏¥‡∏ì - ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏™‡∏≤‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏î‡∏ô‡∏ï‡∏£‡∏µ
            frequency = 440  # A4
            audio = np.sin(2 * np.pi * frequency * t) * np.exp(-t * 2)
            
        elif self.instrument == '‡πÅ‡∏Ñ‡∏ô':
            # ‡πÅ‡∏Ñ‡∏ô - ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏õ‡∏≤‡∏Å‡πÄ‡∏õ‡πà‡∏≤
            frequency = 220  # A3
            audio = np.sin(2 * np.pi * frequency * t) * 0.8
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Æ‡∏≤‡∏£‡πå‡∏°‡∏≠‡∏ô‡∏¥‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏õ‡∏≤‡∏Å‡πÄ‡∏õ‡πà‡∏≤
            audio += 0.3 * np.sin(2 * np.pi * frequency * 2 * t)
            audio += 0.1 * np.sin(2 * np.pi * frequency * 3 * t)
            
        elif self.instrument == '‡πÇ‡∏´‡∏ß‡∏î':
            # ‡πÇ‡∏´‡∏ß‡∏î - ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏Å‡∏•‡∏≠‡∏á
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏∞‡πÅ‡∏ö‡∏ö‡∏≠‡∏µ‡∏™‡∏≤‡∏ô
            rhythm = np.zeros_like(t)
            beat_interval = int(self.sample_rate * 0.5)  # 0.5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏ï‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏∞
            for i in range(0, len(rhythm), beat_interval):
                if i < len(rhythm):
                    rhythm[i] = 1.0
            audio = rhythm * np.exp(-t * 5)  # ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß
            
        elif self.instrument == '‡πÇ‡∏õ‡∏á‡∏•‡∏≤‡∏á':
            # ‡πÇ‡∏õ‡∏á‡∏•‡∏≤‡∏á - ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏°‡πâ‡πÑ‡∏ú‡πà
            frequency = 330  # E4
            audio = np.sin(2 * np.pi * frequency * t) * 0.6
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° resonance ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏°‡πâ
            audio += 0.2 * np.sin(2 * np.pi * frequency * 1.5 * t)
        
        # Normalize and save
        audio = audio / np.max(np.abs(audio))
        audio_tensor = torch.FloatTensor(audio).unsqueeze(0)
        
        sample_path = sample_dir / f"{self.instrument}_sample.wav"
        torchaudio.save(sample_path, audio_tensor, self.sample_rate)
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {sample_path}")
    
    def _preprocess_audio(self, audio: torch.Tensor) -> torch.Tensor:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô"""
        # ‡∏ï‡∏±‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        if audio.shape[0] > self.samples_per_track:
            audio = audio[:self.samples_per_track]
        elif audio.shape[0] < self.samples_per_track:
            padding = self.samples_per_track - audio.shape[0]
            audio = torch.nn.functional.pad(audio, (0, padding))
        
        return audio
    
    def _extract_features(self, audio: torch.Tensor) -> torch.Tensor:
        """‡∏î‡∏∂‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô"""
        # Convert to mel spectrogram
        mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=self.sample_rate,
            n_fft=self.instrument_config.n_fft,
            hop_length=self.instrument_config.hop_length,
            n_mels=self.instrument_config.n_mels
        )
        
        mel_spec = mel_transform(audio)
        mel_spec = torchaudio.transforms.AmplitudeToDB()(mel_spec)
        
        return mel_spec
    
    def __len__(self) -> int:
        return len(self.audio_files)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡∏∞‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå"""
        audio_path = self.audio_files[idx]
        
        # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á
        audio, sr = torchaudio.load(audio_path)
        
        # ‡πÅ‡∏õ‡∏•‡∏á sample rate ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        if sr != self.sample_rate:
            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
            audio = resampler(audio)
        
        # ‡πÉ‡∏ä‡πâ‡πÅ‡∏ä‡∏ô‡πÅ‡∏ô‡∏•‡πÅ‡∏£‡∏Å‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ä‡∏ô‡πÅ‡∏ô‡∏•
        if audio.shape[0] > 1:
            audio = audio[0]
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
        audio = self._preprocess_audio(audio)
        
        # ‡∏î‡∏∂‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
        features = self._extract_features(audio)
        
        # ‡πÉ‡∏ä‡πâ transform ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        if self.transform:
            features = self.transform(features)
        
        return features, audio

class IsanMusicPreprocessor:
    """‡∏ï‡∏±‡∏ß‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô"""
    
    def __init__(self, sample_rate: int = 16000):
        self.sample_rate = sample_rate
    
    def apply_isan_effects(self, audio: torch.Tensor, instrument: str) -> torch.Tensor:
        """‡πÉ‡∏™‡πà‡πÄ‡∏≠‡∏ü‡πÄ‡∏ü‡∏Å‡∏ï‡πå‡πÅ‡∏ö‡∏ö‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô"""
        if instrument == '‡∏û‡∏¥‡∏ì':
            return self._apply_phin_effects(audio)
        elif instrument == '‡πÅ‡∏Ñ‡∏ô':
            return self._apply_khaen_effects(audio)
        elif instrument == '‡πÇ‡∏´‡∏ß‡∏î':
            return self._apply_woad_effects(audio)
        elif instrument == '‡πÇ‡∏õ‡∏á‡∏•‡∏≤‡∏á':
            return self._apply_ponglang_effects(audio)
        return audio
    
    def _apply_phin_effects(self, audio: torch.Tensor) -> torch.Tensor:
        """‡πÄ‡∏≠‡∏ü‡πÄ‡∏ü‡∏Å‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏¥‡∏ì"""
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° vibrato ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
        t = torch.linspace(0, len(audio) / self.sample_rate, len(audio))
        vibrato = 1 + 0.1 * torch.sin(2 * np.pi * 5 * t)  # 5 Hz vibrato
        return audio * vibrato
    
    def _apply_khaen_effects(self, audio: torch.Tensor) -> torch.Tensor:
        """‡πÄ‡∏≠‡∏ü‡πÄ‡∏ü‡∏Å‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Ñ‡∏ô"""
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° breath noise ‡πÅ‡∏•‡∏∞ resonance
        noise = torch.randn_like(audio) * 0.05
        return audio + noise
    
    def _apply_woad_effects(self, audio: torch.Tensor) -> torch.Tensor:
        """‡πÄ‡∏≠‡∏ü‡πÄ‡∏ü‡∏Å‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏ß‡∏î"""
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° reverb ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏Å‡∏•‡∏≠‡∏á
        return audio * 0.9  # ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
    
    def _apply_ponglang_effects(self, audio: torch.Tensor) -> torch.Tensor:
        """‡πÄ‡∏≠‡∏ü‡πÄ‡∏ü‡∏Å‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏õ‡∏á‡∏•‡∏≤‡∏á"""
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° harmonic enhancement
        return audio * 1.1

def create_isan_music_dataloader(
    data_path: str,
    instrument: str,
    batch_size: int = 8,
    num_workers: int = 4,
    **kwargs
) -> torch.utils.data.DataLoader:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô"""
    dataset = IsanMusicDataset(data_path, instrument, **kwargs)
    
    return torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
if __name__ == "__main__":
    print("üéµ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
    dataset = IsanMusicDataset(
        data_path="./isan_audio_data",
        instrument="‡∏û‡∏¥‡∏ì",
        duration=5.0
    )
    
    print(f"üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(dataset)}")
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    if len(dataset) > 0:
        features, audio = dataset[0]
        print(f"üîä ‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: {features.shape}")
        print(f"üé∂ ‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á: {audio.shape}")
        
    print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")